{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFhqrPopctwa"
      },
      "source": [
        "Copyright 2020 DeepMind Technologies Limited.\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "https://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuYtT7GpuVgQ"
      },
      "source": [
        "\u003cp align=\"center\"\u003e\n",
        "  \u003ch1 align=\"center\"\u003eTAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement\u003c/h1\u003e\n",
        "  \u003cp align=\"center\"\u003e\n",
        "    \u003ca href=\"http://www.carldoersch.com/\"\u003eCarl Doersch\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://yangyi02.github.io/\"\u003eYi Yang\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://scholar.google.com/citations?user=Jvi_XPAAAAAJ\"\u003eMel Vecerik\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://scholar.google.com/citations?user=cnbENAEAAAAJ\"\u003eDilara Gokay\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://www.robots.ox.ac.uk/~ankush/\"\u003eAnkush Gupta\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"http://people.csail.mit.edu/yusuf/\"\u003eYusuf Aytar\u003c/a\u003e\n",
        "    \u003ca href=\"https://scholar.google.co.uk/citations?user=IUZ-7_cAAAAJ\"\u003eJoao Carreira\u003c/a\u003e\n",
        "    ·\n",
        "    \u003ca href=\"https://www.robots.ox.ac.uk/~az/\"\u003eAndrew Zisserman\u003c/a\u003e\n",
        "  \u003c/p\u003e\n",
        "  \u003ch3 align=\"center\"\u003e\u003ca href=\"https://arxiv.org/abs/2306.08637\"\u003ePaper\u003c/a\u003e | \u003ca href=\"https://deepmind-tapir.github.io\"\u003eProject Page\u003c/a\u003e | \u003ca href=\"https://github.com/deepmind/tapnet\"\u003eGitHub\u003c/a\u003e | \u003ca href=\"https://github.com/deepmind/tapnet/tree/main#running-tapir-locally\"\u003eLive Demo\u003c/a\u003e \u003c/h3\u003e\n",
        "  \u003cdiv align=\"center\"\u003e\u003c/div\u003e\n",
        "\u003c/p\u003e\n",
        "\n",
        "\u003cp align=\"center\"\u003e\n",
        "  \u003ca href=\"\"\u003e\n",
        "    \u003cimg src=\"https://storage.googleapis.com/dm-tapnet/swaying_gif.gif\" alt=\"Logo\" width=\"50%\"\u003e\n",
        "  \u003c/a\u003e\n",
        "\u003c/p\u003e\n",
        "\u003ch3\u003e\n",
        "\u003c/h3\u003e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCmDvfFvxnGB"
      },
      "outputs": [],
      "source": [
        "# @title Install Code and Dependencies {form-width: \"25%\"}\n",
        "!pip install git+https://github.com/google-deepmind/tapnet.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaswJZMq9B3c"
      },
      "outputs": [],
      "source": [
        "# @title Download Model {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/checkpoints\n",
        "\n",
        "!wget -P tapnet/checkpoints https://storage.googleapis.com/dm-tapnet/bootstap/causal_bootstapir_checkpoint.pt\n",
        "\n",
        "%ls tapnet/checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxlHY242m-6Q"
      },
      "outputs": [],
      "source": [
        "# @title Imports {form-width: \"25%\"}\n",
        "%matplotlib widget\n",
        "from google.colab import output\n",
        "import jax\n",
        "import matplotlib.pyplot as plt\n",
        "import mediapy as media\n",
        "import numpy as np\n",
        "from tapnet.torch import tapir_model\n",
        "from tapnet.utils import transforms\n",
        "from tapnet.utils import viz_utils\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "output.enable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swWFHeHFrsT0"
      },
      "outputs": [],
      "source": [
        "# @title Select Device {form-width: \"25%\"}\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rfy2yobnHqw"
      },
      "outputs": [],
      "source": [
        "# @title Load an Exemplar Video {form-width: \"25%\"}\n",
        "\n",
        "%mkdir tapnet/examplar_videos\n",
        "\n",
        "!wget -P tapnet/examplar_videos https://storage.googleapis.com/dm-tapnet/horsejump-high.mp4\n",
        "\n",
        "video = media.read_video(\"tapnet/examplar_videos/horsejump-high.mp4\")\n",
        "media.show_video(video, fps=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhaB3rBssLLc"
      },
      "outputs": [],
      "source": [
        "# @title Utility Functions {form-width: \"25%\"}\n",
        "\n",
        "\n",
        "def preprocess_frames(frames):\n",
        "  \"\"\"Preprocess frames to model inputs.\n",
        "\n",
        "  Args:\n",
        "    frames: [num_frames, height, width, 3], [0, 255], np.uint8\n",
        "\n",
        "  Returns:\n",
        "    frames: [num_frames, height, width, 3], [-1, 1], np.float32\n",
        "  \"\"\"\n",
        "  frames = frames.float()\n",
        "  frames = frames / 255 * 2 - 1\n",
        "  return frames\n",
        "\n",
        "\n",
        "def sample_random_points(frame_max_idx, height, width, num_points):\n",
        "  \"\"\"Sample random points with (time, height, width) order.\"\"\"\n",
        "  y = np.random.randint(0, height, (num_points, 1))\n",
        "  x = np.random.randint(0, width, (num_points, 1))\n",
        "  t = np.random.randint(0, frame_max_idx + 1, (num_points, 1))\n",
        "  points = np.concatenate((t, y, x), axis=-1).astype(\n",
        "      np.int32\n",
        "  )  # [num_points, 3]\n",
        "  return points\n",
        "\n",
        "\n",
        "def postprocess_occlusions(occlusions, expected_dist):\n",
        "  visibles = (1 - F.sigmoid(occlusions)) * (1 - F.sigmoid(expected_dist)) \u003e 0.5\n",
        "  return visibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7wOMJoSQzq1"
      },
      "outputs": [],
      "source": [
        "# @title Define Model {form-width: \"25%\"}\n",
        "\n",
        "model = tapir_model.TAPIR(pyramid_level=1, use_casual_conv=True)\n",
        "\n",
        "model.load_state_dict(\n",
        "    torch.load('tapnet/checkpoints/causal_bootstapir_checkpoint.pt')\n",
        ")\n",
        "model = model.to(device)\n",
        "model = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JH5jNtQAfTP4"
      },
      "outputs": [],
      "source": [
        "# @title Set to Inference Mode to Save Memory {form-width: \"25%\"}\n",
        "\n",
        "model = model.eval()\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izp33JBg6eij"
      },
      "outputs": [],
      "source": [
        "# @title Inference Functions {form-width: \"25%\"}\n",
        "\n",
        "\n",
        "def online_model_init(frames, query_points):\n",
        "  \"\"\"Initialize query features for the query points.\"\"\"\n",
        "  frames = preprocess_frames(frames)\n",
        "  feature_grids = model.get_feature_grids(frames, is_training=False)\n",
        "  query_features = model.get_query_features(\n",
        "      frames,\n",
        "      is_training=False,\n",
        "      query_points=query_points,\n",
        "      feature_grids=feature_grids,\n",
        "  )\n",
        "  return query_features\n",
        "\n",
        "\n",
        "def online_model_predict(frames, query_features, causal_context):\n",
        "  \"\"\"Compute point tracks and occlusions given frames and query points.\"\"\"\n",
        "  frames = preprocess_frames(frames)\n",
        "  feature_grids = model.get_feature_grids(frames, is_training=False)\n",
        "  trajectories = model.estimate_trajectories(\n",
        "      frames.shape[-3:-1],\n",
        "      is_training=False,\n",
        "      feature_grids=feature_grids,\n",
        "      query_features=query_features,\n",
        "      query_points_in_video=None,\n",
        "      query_chunk_size=64,\n",
        "      causal_context=causal_context,\n",
        "      get_causal_context=True,\n",
        "  )\n",
        "  causal_context = trajectories['causal_context']\n",
        "  del trajectories['causal_context']\n",
        "  # Take only the predictions for the final resolution.\n",
        "  # For running on higher resolution, it's typically better to average across\n",
        "  # resolutions.\n",
        "  tracks = trajectories['tracks'][-1]\n",
        "  occlusions = trajectories['occlusion'][-1]\n",
        "  uncertainty = trajectories['expected_dist'][-1]\n",
        "  visibles = postprocess_occlusions(occlusions, uncertainty)\n",
        "  return tracks, visibles, causal_context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LLK7myqp3Px"
      },
      "outputs": [],
      "source": [
        "# @title Predict Sparse Point Tracks {form-width: \"25%\"}\n",
        "\n",
        "resize_height = 256  # @param {type: \"integer\"}\n",
        "resize_width = 256  # @param {type: \"integer\"}\n",
        "num_points = 50  # @param {type: \"integer\"}\n",
        "\n",
        "frames = media.resize_video(video, (resize_height, resize_width))\n",
        "query_points = sample_random_points(\n",
        "    0, frames.shape[1], frames.shape[2], num_points\n",
        ")\n",
        "frames = torch.tensor(frames).to(device)\n",
        "query_points = torch.tensor(query_points).to(device)\n",
        "\n",
        "# Extract features for the query point at the first frame.\n",
        "query_features = online_model_init(frames[None, 0:1], query_points[None])\n",
        "# If there are query points at other frames, we need to call:\n",
        "# query_features = online_model_init(frames[None], query_points[None])\n",
        "\n",
        "causal_state = model.construct_initial_causal_state(\n",
        "    query_points.shape[0], len(query_features.resolutions) - 1\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(len(causal_state)):\n",
        "    for k, v in causal_state[i].items():\n",
        "      causal_state[i][k] = v.to(device)\n",
        "\n",
        "  # Predict point tracks frame by frame\n",
        "  predictions = []\n",
        "  for i in range(frames.shape[0]):\n",
        "    # Note: we add a batch dimension.\n",
        "    tracks, visibles, causal_state = online_model_predict(\n",
        "        frames=frames[None, i : i + 1],\n",
        "        query_features=query_features,\n",
        "        causal_context=causal_state,\n",
        "    )\n",
        "    predictions.append({'tracks': tracks, 'visibles': visibles})\n",
        "\n",
        "tracks = torch.cat([x['tracks'][0] for x in predictions], dim=1)\n",
        "visibles = torch.cat([x['visibles'][0] for x in predictions], dim=1)\n",
        "\n",
        "tracks = tracks.cpu().numpy()\n",
        "visibles = visibles.cpu().numpy()\n",
        "# Visualize sparse point tracks\n",
        "height, width = video.shape[1:3]\n",
        "tracks = transforms.convert_grid_coordinates(\n",
        "    tracks, (resize_width, resize_height), (width, height)\n",
        ")\n",
        "video_viz = viz_utils.paint_point_track(video, tracks, visibles)\n",
        "media.show_video(video_viz, fps=10)"
      ]
    },
    {
      "metadata": {
        "id": "GiGyAJkFWcJA"
      },
      "cell_type": "code",
      "source": [
        "# @title Download TAPVid-DAVSIS Dataset {form-width: \"25%\"}\n",
        "!wget https://storage.googleapis.com/dm-tapnet/tapvid_davis.zip\n",
        "!unzip tapvid_davis.zip"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "znfIrqu9WcJA"
      },
      "cell_type": "code",
      "source": [
        "# @title DAVIS First Eval on 256x256 Resolution {form-width: \"25%\"}\n",
        "%%time\n",
        "\n",
        "from tapnet import evaluation_datasets\n",
        "\n",
        "davis_dataset = evaluation_datasets.create_davis_dataset(\n",
        "    'tapvid_davis/tapvid_davis.pkl', query_mode='first'\n",
        ")\n",
        "\n",
        "summed_scalars = None\n",
        "for sample_idx, sample in enumerate(davis_dataset):\n",
        "  sample = sample['davis']\n",
        "  frames = np.round((sample['video'][0] + 1) / 2 * 255).astype(np.uint8)\n",
        "  query_points = sample['query_points'][0]\n",
        "\n",
        "  frames = torch.tensor(frames).to(device)\n",
        "  query_points = torch.tensor(query_points).to(device)\n",
        "\n",
        "  query_features = online_model_init(frames[None], query_points[None])\n",
        "\n",
        "  causal_state = model.construct_initial_causal_state(\n",
        "      query_points.shape[0], len(query_features.resolutions) - 1\n",
        "  )\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i in range(len(causal_state)):\n",
        "      for k, v in causal_state[i].items():\n",
        "        causal_state[i][k] = v.to(device)\n",
        "\n",
        "    # Predict point tracks frame by frame\n",
        "    predictions = []\n",
        "    for i in range(frames.shape[0]):\n",
        "      # Note: we add a batch dimension.\n",
        "      tracks, visibles, causal_state = online_model_predict(\n",
        "          frames=frames[None, i : i + 1],\n",
        "          query_features=query_features,\n",
        "          causal_context=causal_state,\n",
        "      )\n",
        "      predictions.append({'tracks': tracks, 'visibles': visibles})\n",
        "\n",
        "  tracks = torch.cat([x['tracks'][0] for x in predictions], dim=1)\n",
        "  visibles = torch.cat([x['visibles'][0] for x in predictions], dim=1)\n",
        "\n",
        "  tracks = tracks.cpu().numpy()\n",
        "  visibles = visibles.cpu().numpy()\n",
        "  occluded = ~visibles\n",
        "\n",
        "  query_points = sample['query_points'][0]\n",
        "\n",
        "  scalars = evaluation_datasets.compute_tapvid_metrics(\n",
        "      query_points[None],\n",
        "      sample['occluded'],\n",
        "      sample['target_points'],\n",
        "      occluded[None],\n",
        "      tracks[None],\n",
        "      query_mode='first',\n",
        "  )\n",
        "  scalars = jax.tree.map(lambda x: np.array(np.sum(x, axis=0)), scalars)\n",
        "  print(sample_idx, scalars)\n",
        "\n",
        "  if summed_scalars is None:\n",
        "    summed_scalars = scalars\n",
        "  else:\n",
        "    summed_scalars = jax.tree.map(np.add, summed_scalars, scalars)\n",
        "\n",
        "  num_samples = sample_idx + 1\n",
        "  mean_scalars = jax.tree.map(lambda x: x / num_samples, summed_scalars)\n",
        "  print(mean_scalars)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "Zl_M0JQymga7"
      },
      "cell_type": "code",
      "source": [
        "# @title Select Any Points at Any Frame {form-width: \"25%\"}\n",
        "\n",
        "select_frame = 0  # @param {type:\"slider\", min:0, max:49, step:1}\n",
        "\n",
        "# Generate a colormap with 20 points, no need to change unless select more than 20 points\n",
        "colormap = viz_utils.get_colors(20)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.imshow(video[select_frame])\n",
        "ax.axis('off')\n",
        "ax.set_title(\n",
        "    'You can select more than 1 point. After selecting enough points, run the'\n",
        "    ' next cell.'\n",
        ")\n",
        "\n",
        "select_points = []\n",
        "\n",
        "\n",
        "# Event handler for mouse clicks\n",
        "def on_click(event):\n",
        "  if event.button == 1 and event.inaxes == ax:  # Left mouse button clicked\n",
        "    x, y = int(np.round(event.xdata)), int(np.round(event.ydata))\n",
        "\n",
        "    select_points.append(np.array([x, y]))\n",
        "\n",
        "    color = colormap[len(select_points) - 1]\n",
        "    color = tuple(np.array(color) / 255.0)\n",
        "    ax.plot(x, y, 'o', color=color, markersize=5)\n",
        "    plt.draw()\n",
        "\n",
        "\n",
        "fig.canvas.mpl_connect('button_press_event', on_click)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "metadata": {
        "id": "eu1fWiN6mga7"
      },
      "cell_type": "code",
      "source": [
        "# @title Predict Point Tracks for the Selected Points {form-width: \"25%\"}\n",
        "\n",
        "resize_height = 256  # @param {type: \"integer\"}\n",
        "resize_width = 256  # @param {type: \"integer\"}\n",
        "\n",
        "\n",
        "def convert_select_points_to_query_points(frame, points):\n",
        "  \"\"\"Convert select points to query points.\n",
        "\n",
        "  Args:\n",
        "    points: [num_points, 2], [t, y, x]\n",
        "\n",
        "  Returns:\n",
        "    query_points: [num_points, 3], [t, y, x]\n",
        "  \"\"\"\n",
        "  points = np.stack(points)\n",
        "  query_points = np.zeros(shape=(points.shape[0], 3), dtype=np.float32)\n",
        "  query_points[:, 0] = frame\n",
        "  query_points[:, 1] = points[:, 1]\n",
        "  query_points[:, 2] = points[:, 0]\n",
        "  return query_points\n",
        "\n",
        "\n",
        "frames = media.resize_video(video, (resize_height, resize_width))\n",
        "query_points = convert_select_points_to_query_points(\n",
        "    select_frame, select_points\n",
        ")\n",
        "height, width = video.shape[1:3]\n",
        "query_points = transforms.convert_grid_coordinates(\n",
        "    query_points,\n",
        "    (1, height, width),\n",
        "    (1, resize_height, resize_width),\n",
        "    coordinate_format='tyx',\n",
        ")\n",
        "\n",
        "frames = torch.tensor(frames).to(device)\n",
        "query_points = torch.tensor(query_points).to(device)\n",
        "\n",
        "query_features = online_model_init(frames[None], query_points[None])\n",
        "causal_state = model.construct_initial_causal_state(\n",
        "    query_points.shape[0], len(query_features.resolutions) - 1\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "  for i in range(len(causal_state)):\n",
        "    for k, v in causal_state[i].items():\n",
        "      causal_state[i][k] = v.to(device)\n",
        "\n",
        "  # Predict point tracks frame by frame\n",
        "  predictions = []\n",
        "  for i in range(frames.shape[0]):\n",
        "    # Note: we add a batch dimension.\n",
        "    tracks, visibles, causal_state = online_model_predict(\n",
        "        frames=frames[None, i : i + 1],\n",
        "        query_features=query_features,\n",
        "        causal_context=causal_state,\n",
        "    )\n",
        "    predictions.append({'tracks': tracks, 'visibles': visibles})\n",
        "\n",
        "tracks = torch.cat([x['tracks'][0] for x in predictions], dim=1)\n",
        "visibles = torch.cat([x['visibles'][0] for x in predictions], dim=1)\n",
        "\n",
        "tracks = tracks.cpu().numpy()\n",
        "visibles = visibles.cpu().numpy()\n",
        "# Visualize sparse point tracks\n",
        "tracks = transforms.convert_grid_coordinates(\n",
        "    tracks, (resize_width, resize_height), (width, height)\n",
        ")\n",
        "video_viz = viz_utils.paint_point_track(video, tracks, visibles, colormap)\n",
        "media.show_video(video_viz, fps=10)"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8orfQRoaRJit"
      },
      "source": [
        "That's it!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "https://github.com/deepmind/tapnet/blob/master/colabs/torch_causal_tapir_demo.ipynb",
          "timestamp": 1720431514861
        }
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
